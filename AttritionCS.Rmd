---
title: "DDS CS2"
author: "Allen Hoskins"
date: "7/21/2021"
output:
  html_document: default
---

# Talent Management Attrition Case Study
In this case study, I analyzed a data set with multiple potential features to determine which features were highly telling in attrition rates. Out of the 840 records given, there were 140 that landed in the numerator for attrition rates. In the first two chunks of code, I loaded int the libraries that would be used for this case study and created several plots to determine contributing factors

## Contributing Factors
Some of the most correlated data points in attrition rates were:
  Monthly income/Monthly Rate
  Job Level
  Overtime
  Age

## Attrition Predictive Modeling
Once I was able to figure out several data points that were highly correlated with attrition rates, I ran several predictive models (KNN, Naive Bayes, and Random Forest) to predict if an employee would fall in to the denominator for attrition rates. 

### Knn Model:
This model performed fairly well in accuracy (84%) and Sensitivity (99.2%), but failed to have high specificity rates. This was concerning as there were a lot of false positives in this modle

### Naive Bayes:
Similarly to the KNN model, the Naive Bayes model performed well in accuracy (85%), but the sensitivity and specificity were opposite of the KNN model. The Naive Bayes model was able to accurately predict the employees who were not in the numerator of the attrition rate, but was very inaccurate in that it predicted a lot of false positives.

### Random Forest:
The random forest model was by far the best model that I ran for predicting attrition rates. It's accuracy (80.8%) was slightly lower than both the KNN and Naive Bayes models, but it's sensitivity and specificity were drastically higher (61.5% and 84.2% respectfully). This meant that this model was able to more accurately predict true positives and true negatives while not sacrificing accuracy for false positives and false negatives

## Salary Predictive Modeling:
I then ran two separate models for predicting salaries for employees. The highest correlating fields were Job Level, Total Working Years and Age. This makes sense as the older you are, the more years you would have worked and your job level is usually higher.

### Random Forest:
When running a random forest model using the fields provided above, I was able to return a RMSE of 1242.69. This was slightly better than the RMSE when using all fields in this model (RMSE of 1278.82)

### Linear Regression:
I then wanted to compare the Random Forest model to a linear regression model, because salary theoretically should be linearly correlated with age, job level and years working. However I found that this models RMSE (1333.81) was slightly higher than the Random Forest model and therefore did not predict salary as accurate.


# Chunk 1: loading in packages used in markdown file
```{r}
library(tidyverse)
library(caret)
library(ggplot2)
library(magrittr)
library(ggExtra)
library(e1071)
library(class)
library(ggthemes)
library(naniar)
library(PerformanceAnalytics)
library(corrplot)
library(RColorBrewer)
library(funModeling)
library(gridExtra)
library(stringr)
library(plyr)
library(janitor)
library(devtools)
library(rpart)
library(caTools)
library(randomForest)
library(lattice)
library(rmarkdown)
library(scales)
library(readxl)
library(knitr)
```

# Chunk 2: Loading in full file and looking at basic summary statistics
```{r}

# reading in data
df = read_csv('~/Desktop/MSDS/Doing Data Science/MSDS_6306_Doing-Data-Science-Master/Unit 14 and 15 Case Study 2/CaseStudy2-data.csv')

#looking at high level summary stats of columns
head(df)
summary(df)
str(df)

# checking for null values in columns
colSums(is.na(df))
gg_miss_var(df,show_pct = TRUE)

# normalizing column
df = df %>% mutate(BusinessTravel = ifelse(BusinessTravel == 'Non_Travel','No','Yes'))

```

# Chunk 3: EDA Graphs
## In this chunk, I several ggplots to determine which fields would best determine employee attrition.
```{r}

df %>% ggplot()+
  geom_boxplot(aes(x=Attrition, y=MonthlyIncome, fill=Attrition), alpha=0.7)+
  ggtitle("Attrition by Monthly Income")+
  scale_y_continuous(name='Monthly Income')+
  theme(plot.title=element_text(size=20, color="Black"))+
  scale_fill_manual(values=c("Green","Red"))

df %>% ggplot()+
  geom_boxplot(aes(x=Attrition, y=MonthlyRate, fill = Attrition), alpha=0.7)+
  ggtitle("Attrition by Monthly Rate")+
  scale_y_continuous(name="Monthly Rate")+
  scale_fill_manual(values=c("Green","Red"))+
  theme(plot.title=element_text(size=20, color="Black"))

# Environment Satisfaction v Attrition
df %>% ggplot()+
  geom_boxplot(aes(x=Attrition, y=EnvironmentSatisfaction, fill=Attrition), alpha=0.7)+
  ggtitle("Attrition by Envoronment Satisfaction")+
  scale_y_continuous(name='Environment Satisfaction')+
  theme(plot.title=element_text(size=20, color="Black"))+
  scale_fill_manual(values=c("Green","Red"))


df %>% ggplot()+
  geom_point(aes(x=Age, y=MonthlyRate,col = Attrition), alpha=0.7)+
  ggtitle("Monthly Rate v Age by Attrition")+
  scale_y_continuous(name='Monthly Rate')+
  theme(plot.title=element_text(size=20, color='Black'))+
  scale_color_manual(values=c("Green","Red"))+
  facet_wrap(vars(JobLevel))


df %>% ggplot()+
  geom_point(aes(x=JobLevel, y=DailyRate, color=Attrition), alpha=0.7)+
  ggtitle("Job Level vs Daily Rate")+
  scale_y_continuous(name="Years in Current Role")+
  theme(plot.title=element_text(size=20, color="Black")) +
  scale_color_manual(values=c("Green","Red"))

 df %>% ggplot()+
  geom_point(aes(x=DailyRate, y=YearsInCurrentRole, color=Attrition), alpha=0.7,position = 'jitter')+
  ggtitle("Daily Rate vs Years in Current Role")+
  scale_y_continuous(name="Years in Current Role")+
  theme(plot.title=element_text(size=20, color="Black")) +
  scale_color_manual(values=c("Green","Red"))

df %>% ggplot()+
  geom_point(aes(x=NumCompaniesWorked ,y =YearsInCurrentRole ,color = Attrition),alpha = 0.7,position = 'jitter')+
  ggtitle('Number of Companies Worked vs Years in Current Role')+
    scale_color_manual(values=c("Green","Red"))

df %>% ggplot()+
  geom_point(aes(x=YearsInCurrentRole ,y = YearsWithCurrManager ,color = Attrition),alpha = 0.7,position = 'jitter')+
  ggtitle('Years in Current Role vs Years with Current Manager')

# Relationship between attrition and overtime
# overtime = df%>%
#   group_by(OverTime)%>%
#   count(Attrition)%>%
#   mutate(AttritionRate=percent(n/sum(n)))
# overtime
# 
# df %>% ggplot(aes(x=OverTime, fill=Attrition))+
#   geom_bar(alpha=0.7)+
#   geom_text(data=overtime, aes(y=n,label=AttritionRate), position=position_stack(vjust=0.5), size=3)+
#   ggtitle("Attrition by Overtime")+
#   scale_x_discrete(name="Overtime")+
#   scale_y_continuous(name="Number of Employees")+
#   theme(plot.title=element_text(size=20, color="Black"))+
#   scale_fill_manual(values=c("Green", "Red"))

# Relationship between monthly income, work life balance, and attrition

 df %>%
  mutate(Attrition=ifelse(Attrition=="Yes"," Yes (140)","No (730)"))%>%
ggplot()+
  geom_boxplot(aes(x=Attrition, y=MonthlyIncome, fill=Attrition), alpha=0.7)+
  ggtitle("Attrition by Monthly Income")+
  scale_y_continuous(name='Monthly Income')+
  theme(plot.title=element_text(size=20, color="Black"))+
  scale_fill_manual(values=c("Green","Red"))

 # Determining if Attrition is based off of Income (Monthly Rate and Income)
p = df %>%
  mutate(Attrition=ifelse(Attrition=="Yes"," Yes (140)","No (730)"))%>%
  ggplot()+
  geom_boxplot(aes(x=Attrition, y=MonthlyIncome, fill=Attrition), alpha=0.7)+
  ggtitle("Attrition by Monthly Income")+
  scale_y_continuous(name='Monthly Income')+
  theme(plot.title=element_text(size=15, color="Black"))+
  scale_fill_manual(values=c("Green","Red"))
p

p0 = df %>%
  mutate(Attrition=ifelse(Attrition=="Yes"," Yes (140)","No (730)"))%>%
  ggplot()+
  geom_boxplot(aes(x=Attrition, y=MonthlyRate, fill = Attrition), alpha=0.7)+
  ggtitle("Attrition by Monthly Rate")+
  scale_y_continuous(name="Monthly Rate")+
  theme(plot.title=element_text(size=15, color="Black"))+
scale_fill_manual(values=c("Green","Red"))
p0

grid.arrange(p,p0)


#Attrition based on job level
# 
#  level = df%>%
#    group_by(JobLevel)%>%
#    count(Attrition)%>%
#    mutate(AttritionRate=percent(n/sum(n)))
#  level
# 
#  df %>%
#    ggplot( aes(x=JobLevel, fill=Attrition))+
#    geom_bar(alpha=0.7)+
#    geom_text(data=level, aes(y=n,label=AttritionRate), position=position_stack(vjust=0.5), size=3)+
#    ggtitle("Attrition by Job Level")+
#    scale_x_discrete(name="Job Level", labels = df$JobLevel)+
#    scale_y_continuous(name="# of employees")+
#    theme(plot.title=element_text(size=20, color="Black"))+
#    scale_fill_manual(values=c("Green", "Red"))
# 
# 
#  p1 = df %>%
#    ggplot( aes(x=JobLevel, fill=Attrition))+
#    geom_bar(alpha=0.7)+
#    geom_text(data=level, aes(y=n,label=AttritionRate), position=position_stack(vjust=0.5), size=3)+
#    ggtitle("Attrition by Job Level")+
#    scale_x_discrete(name="Job Level", labels = df$JobLevel)+
#    scale_y_continuous(name="# of employees")+
#    theme(plot.title=element_text(size=20, color="Black"))+
#    scale_fill_manual(values=c("Green", "Red"))+
#  p1
#  grid.arrange(p,p1)
#  p2 = df %>%
#    ggplot(aes(x=OverTime, fill=Attrition))+
#    geom_bar(alpha=0.7)+
#    geom_text(data=overtime, aes(y=n,label=AttritionRate), position=position_stack(vjust=0.5), size=3)+
#    ggtitle("Attrition by Overtime")+
#    scale_x_discrete(name="Overtime")+
#    scale_y_continuous(name="Number of Employees")+
#    theme(plot.title=element_text(size=20, color="Black"))+
#    scale_fill_manual(values=c("Green", "Red"))
#  p2
#  grid.arrange(p1,p2)
```


# Chunk 4: KNN Model
## In this chunk, I ran a KNN classification model which had poor specificity, meaning that the false negatives are extremely high. Hover the accuracy was extremly high at 84% and the Sensitivity was 99.59%.

```{r}

dfKNN= read.csv('~/Desktop/MSDS/Doing Data Science/MSDS_6306_Doing-Data-Science-Master/Unit 14 and 15 Case Study 2/CaseStudy2-data.csv',header = TRUE)

set.seed(57)

splitPerc = .70
trainAttrKNN= sample(1:dim(dfKNN)[1],round(splitPerc * dim(dfKNN)[1]))
trainKNN = dfKNN[trainAttrKNN,]
testKNN = dfKNN[-trainAttrKNN,]



#creation of KNN model using leave one out method
# classification = knn.cv(dfKNN[,c(2,6,14,16,18,20,22)],dfKNN$Attrition,prob = TRUE, k = 9)
# table(classification,dfKNN$Attrition)
# confusionMatrix(table(classification,dfKNN$Attrition))


# determining best k 

set.seed(57)
#running KNN model 90 times to find best k parameter 
accs = data.frame(accuracy = numeric(90), k = numeric(90))

for(i in 1:90)
{
   classification = knn.cv(dfKNN[,c(2,14,16,18,20,22)],dfKNN$Attrition,prob = TRUE, k = i)
   table(classification,dfKNN$Attrition)
   CM = confusionMatrix(table(classification,dfKNN$Attrition))
  accs$accuracy[i] = CM$overall[1]
  accs$k[i] = i
}

plot(accs$k,accs$accuracy, type = "l", xlab = "k")
abline(v=accs$k[which.max(accs$accuracy)], col="red")
accs$k[which.max(accs$accuracy)]

set.seed(57)

#use tuned parameter from code above
classification = knn.cv(df[,c(2,14,16,18,20,22)],dfKNN$Attrition,prob = TRUE, k = 19)
table(classification,dfKNN$Attrition)
confusionMatrix(table(classification,dfKNN$Attrition))

```


# Chunk 5: Naive Bayes 
## In this chunk, I ran a Naive Bayes  model. This model had similar accuracy to the KNN model run above (85%), but it's specificity was 98% compared to the KNN of 4.2%. The Naive Bayes model had a Sensitivity of 14.6%, while the KNN model's Sensitivity was 99.17%.

```{r}

set.seed(57)

dfNB = read.csv("~/Desktop/MSDS/Doing Data Science/MSDS_6306_Doing-Data-Science-Master/Unit 14 and 15 Case Study 2/CaseStudy2-data.csv", header = TRUE)
dfNBNO= read.csv("~/Desktop/MSDS/Doing Data Science/MSDS_6306_Doing-Data-Science-Master/Unit 14 and 15 Case Study 2/CaseStudy2CompSet No Attrition.csv", header = TRUE)

dfNB$Attrition =  factor(as.character(dfNB$Attrition), levels=c('Yes', 'No'))

#70/30 Split of data
splitPerc = .70
trainAttrNB= sample(1:dim(dfNB)[1],round(splitPerc * dim(dfNB)[1]))
trainNB = dfNB[trainAttrNB,]
testNB = dfNB[-trainAttrNB,]


model = naiveBayes(trainNB[,c(2,4,5,15,20)],trainNB$Attrition)
table(predict(model,testNB[,c(2,4,5,15,20)]),testNB$Attrition)
confusionMatrix(table(predict(model,testNB[,c(2,4,5,15,20)]),testNB$Attrition))

```

# Chunk 6: Random Forest Predicting Attrition
## In this chunk I ran a Random Forest model, which performed better than both the KNN and Random Forest models. The Accuracy was slightly lower at 80.8%, but Sensitivity and Specificity were both over 60%, 61.5% and 84.2% respectifully.
```{r}

set.seed(57)

dfRF = read.csv("~/Desktop/MSDS/Doing Data Science/MSDS_6306_Doing-Data-Science-Master/Unit 14 and 15 Case Study 2/CaseStudy2-data.csv", header = TRUE)
dfRFNO= read.csv("~/Desktop/MSDS/Doing Data Science/MSDS_6306_Doing-Data-Science-Master/Unit 14 and 15 Case Study 2/CaseStudy2CompSet No Attrition.csv", header = TRUE)

#Ensure Attrition is changed to factor
dfRF$Attrition =  factor(as.character(dfRF$Attrition), levels=c('Yes', 'No'))

#Split test and train data - 70/30
splitPerc = .70
trainAttrition= sample(1:dim(dfRF)[1],round(splitPerc * dim(dfRF)[1]))
trainRF = dfRF[trainAttrition,]
testRF = dfRF[-trainAttrition,]

#Apply Random Forest using Monthly Income to test data
EmpAttRF = randomForest(Attrition ~ .-MonthlyRate,
                      data=trainRF, 
                      strata=trainRF$Attrition,
                      sampsize= c(55,55))

#Use newly trained data set to predict test set
AttPredRF = predict(EmpAttRF,newdata= testRF)

#Create confusion matrix to assess accuracy stats
confusionMatrix(AttPredRF, testRF$Attrition)

#checking importance of variables
varImp(EmpAttRF)
varImpPlot(EmpAttRF)

#Apply Random Forest to the output file
EmpAttRF2 = randomForest(Attrition ~ .-MonthlyRate,
                       data=dfRF,
                       strata=dfRF$Attrition,
                       sampsize= c(55,55))

AttPredRF2 = predict(EmpAttRF2,newdata= dfRFNO)
EmpAttrPredRF = data.frame(dfRFNO$ID, AttPredRF2)
AttPredRF2

write.csv(AttPredRF2, "~/Desktop/MSDS/Doing Data Science/MSDS_6306_Doing-Data-Science-Master/Unit 14 and 15 Case Study 2/Case2PredictionsHoskinsAttrition.csv")

```

# Chunk 7: Random Forest Model for Salary Prediction
##  In this chunk I ran a Random Forest  model to predict Salary for associates. I started off using all fields and determined the variable importance using varIMP function. After determining which fiels were best for this model(JobLevel, TotalWorkingYears, and Age), I reran the model to achice a RMSE of 1278.822. 
```{r}

#Random Forest for Predicting Salary

SalaryRF = randomForest(MonthlyIncome ~ .-MonthlyRate, data=trainRF)

rfPredictSalary = predict(SalaryRF, newdata= testRF)
rfPredictSalary

RMSE(testRF$MonthlyIncome, rfPredictSalary)
plot(testRF$MonthlyIncome, rfPredictSalary, type = "p", main= "Random Forest RMSE for Predicting Salary", xlab= "Monthly Income", ylab= "Monthly Income Prediction")

varImp(SalaryRF)
varImpPlot(SalaryRF)

#random forest model 2: Updating inputs

SalaryRF2 = randomForest(MonthlyIncome ~ JobLevel+TotalWorkingYears+Age, data=trainRF)

rfPredictSalary2 = predict(SalaryRF2, newdata= testRF)
rfPredictSalary2

varImp(SalaryRF2)
varImpPlot(SalaryRF2)

#RMSE of Random Forest model for predicting salary

RMSE(testRF$MonthlyIncome, rfPredictSalary2)
plot(testRF$MonthlyIncome, rfPredictSalary2, type = "p", main= "Random Forest RMSE for Predicting Salary", xlab= "Monthly Income", ylab= "Monthly Income Prediction")

```

# Chunk 8: Linear Regression for Salary Model
## In this chunk I ran an linear regression model to predict salary. When running this LR model, I was able to return a RMSE of 1333.81. This model performed similarly to the Random Forest model, but was slighly less accurate.
```{r}
#Linear regression model

SalaryRM =lm(MonthlyIncome ~ JobLevel+TotalWorkingYears+Age, data=trainRF)
lmPredictSalary= predict(SalaryRM, newdata= testRF)
lmPredictSalary

RMSE(testRF$MonthlyIncome, lmPredictSalary)
plot(testRF$MonthlyIncome, lmPredictSalary, type = "p", main= "Linear Regression Model for Predicting Salary", xlab= "Monthly Income", ylab= "Monthly Income Prediction")
abline(SalaryRM)
summary(lmPredictSalary)

```

# Chunk 9: Running RF Model for Salary Prediction
## In this chunk, I ran the dataset with No Salary information in it against my tuned Random Forest model.
```{r}
#Load in Data set with no salary information 

NOSalary = read_xlsx('~/Desktop/MSDS/Doing Data Science/MSDS_6306_Doing-Data-Science-Master/Unit 14 and 15 Case Study 2/CaseStudy2CompSet No Salary.xlsx')

#Apply Random Forest to the output file
SalPredRFFinal = predict(SalaryRF2,
                      newdata= NOSalary)
Case2PredictionsHoskinsSalary = data.frame(NOSalary$ID, SalPredRFFinal)
Case2PredictionsHoskinsSalary

```

# Chunk 10: Writing Prediction to CSV
```{r}
write.csv(Case2PredictionsHoskinsSalary, "~/Desktop/MSDS/Doing Data Science/MSDS_6306_Doing-Data-Science-Master/Unit 14 and 15 Case Study 2/Case2PredictionsHoskinsSalary.csv")
```